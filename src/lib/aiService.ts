// AI Service - Handles OpenAI and Gemini API calls
// Supports GPT-5.2, GPT-4o, Gemini 3 Pro, Gemini 2.5, etc.

export interface AIConfig {
    provider: 'openai' | 'gemini';
    apiKey: string;
    model: string;
}

export interface AIResponse {
    success: boolean;
    content?: string;
    error?: string;
}

// Get current AI configuration from localStorage
export const getAIConfig = (): AIConfig | null => {
    const provider = localStorage.getItem('ai_provider') as 'openai' | 'gemini' | null;
    const apiKey = localStorage.getItem('ai_api_key');
    const model = localStorage.getItem('ai_model');

    console.log('[AI Config] Provider:', provider, 'Model:', model, 'API Key exists:', !!apiKey);

    if (!provider || !apiKey || apiKey.length < 10) {
        console.warn('[AI Config] Missing config - provider:', provider, 'apiKey length:', apiKey?.length);
        return null;
    }

    // Migrate invalid model names to valid ones
    let finalModel = model || getDefaultModel(provider);

    // List of valid models - ì‹¤ì œ APIì—ì„œ ì‘ë™í•˜ëŠ” ëª¨ë¸ëª…
    const validOpenAIModels = [
        'gpt-5.2-pro', 'gpt-5.2-chat-latest', 'gpt-5.2',  // GPT-5 ì‹œë¦¬ì¦ˆ
        'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-4',  // GPT-4 ì‹œë¦¬ì¦ˆ
        'gpt-3.5-turbo'  // ë ˆê±°ì‹œ
    ];
    const validGeminiModels = [
        'gemini-2.5-pro-preview-05-06', 'gemini-2.5-flash-preview-05-20',  // Gemini 2.5
        'gemini-2.0-flash', 'gemini-2.0-flash-exp',  // Gemini 2.0
        'gemini-1.5-pro', 'gemini-1.5-flash'  // Gemini 1.5
    ];

    if (provider === 'openai' && !validOpenAIModels.includes(finalModel)) {
        console.warn('[AI Config] Invalid OpenAI model:', finalModel, '- resetting to gpt-4o');
        finalModel = 'gpt-4o';
        localStorage.setItem('ai_model', finalModel);
        localStorage.setItem('openai_model', finalModel);
    }

    if (provider === 'gemini' && !validGeminiModels.includes(finalModel)) {
        console.warn('[AI Config] Invalid Gemini model:', finalModel, '- resetting to gemini-1.5-pro');
        finalModel = 'gemini-1.5-pro';
        localStorage.setItem('ai_model', finalModel);
        localStorage.setItem('gemini_model', finalModel);
    }

    console.log('[AI Config] Using model:', finalModel);

    return { provider, apiKey, model: finalModel };
};

const getDefaultModel = (provider: 'openai' | 'gemini'): string => {
    return provider === 'openai' ? 'gpt-4o' : 'gemini-2.5-flash';
};

// Validate API key by making a test request
export const validateApiKey = async (provider: 'openai' | 'gemini', apiKey: string, model: string): Promise<AIResponse> => {
    try {
        if (provider === 'openai') {
            return await callOpenAI(apiKey, model, 'Say "OK" if you can hear me.');
        } else {
            return await callGemini(apiKey, model, 'Say "OK" if you can hear me.');
        }
    } catch (error: any) {
        return { success: false, error: error.message || 'API validation failed' };
    }
};

// Call OpenAI API
export const callOpenAI = async (apiKey: string, model: string, prompt: string): Promise<AIResponse> => {
    try {
        // Use model directly - SettingsModal now uses real OpenAI model names
        const actualModel = model || 'gpt-4o';

        // GPT-5.2 ë° ìµœì‹  ëª¨ë¸ì€ max_completion_tokens ì‚¬ìš©
        const isNewModel = actualModel.startsWith('gpt-5') || actualModel.startsWith('o1') || actualModel.startsWith('o3') || actualModel.startsWith('o4');
        const tokenParams = isNewModel
            ? { max_completion_tokens: 2000 }
            : { max_tokens: 2000 };

        const response = await fetch('https://api.openai.com/v1/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${apiKey}`
            },
            body: JSON.stringify({
                model: actualModel,
                messages: [{ role: 'user', content: prompt }],
                ...tokenParams,
                temperature: 0.7
            })
        });

        if (!response.ok) {
            const errorData = await response.json().catch(() => ({}));
            throw new Error(errorData.error?.message || `OpenAI API error: ${response.status}`);
        }

        const data = await response.json();
        const content = data.choices?.[0]?.message?.content || '';
        return { success: true, content };
    } catch (error: any) {
        console.error('[AI Service] OpenAI error:', error);
        return { success: false, error: error.message || 'OpenAI API call failed' };
    }
};

// Call Gemini API
export const callGemini = async (apiKey: string, model: string, prompt: string): Promise<AIResponse> => {
    try {
        // Use model directly - SettingsModal now uses real Gemini model names
        const actualModel = model || 'gemini-1.5-flash';

        const response = await fetch(
            `https://generativelanguage.googleapis.com/v1beta/models/${actualModel}:generateContent?key=${apiKey}`,
            {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    contents: [{ parts: [{ text: prompt }] }],
                    generationConfig: {
                        temperature: 0.7,
                        maxOutputTokens: 2000
                    }
                })
            }
        );

        if (!response.ok) {
            const errorData = await response.json().catch(() => ({}));
            throw new Error(errorData.error?.message || `Gemini API error: ${response.status}`);
        }

        const data = await response.json();
        const content = data.candidates?.[0]?.content?.parts?.[0]?.text || '';
        return { success: true, content };
    } catch (error: any) {
        console.error('[AI Service] Gemini error:', error);
        return { success: false, error: error.message || 'Gemini API call failed' };
    }
};

// Generate AI insights report
export const generateAIReport = async (
    clips: any[],
    relatedClips: any[],
    startDate: string,
    endDate: string,
    language: 'ko' | 'en'
): Promise<AIResponse> => {
    const config = getAIConfig();
    if (!config) {
        return { success: false, error: 'AI not configured' };
    }

    const clipSummaries = clips.map((clip, i) =>
        `[${i + 1}] Title: ${clip.title || clip.url}\n    Date: ${new Date(clip.createdAt?.seconds ? clip.createdAt.seconds * 1000 : clip.createdAt).toISOString().split('T')[0]}\n    Keywords: ${(clip.tags || clip.keywords || []).join(', ')}\n    Summary: ${clip.summary || clip.description || ''}`
    ).join('\n\n');

    const relatedSummaries = relatedClips.map((clip, i) =>
        `[R-${i + 1}] Title: ${clip.title || clip.url}\n    Date: ${new Date(clip.createdAt?.seconds ? clip.createdAt.seconds * 1000 : clip.createdAt).toISOString().split('T')[0]}\n    Keywords: ${(clip.tags || clip.keywords || []).join(', ')}\n    Summary: ${clip.summary || clip.description || ''}`
    ).join('\n\n');

    const prompt = language === 'ko'
        ? `
[ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ ìƒì„± ìš”ì²­]

ë‹¤ìŒì€ ì‚¬ìš©ìê°€ ì„ íƒí•œ ê¸°ê°„(${startDate} ~ ${endDate}) ë™ì•ˆ ì €ì¥í•œ í´ë¦½ ëª©ë¡ì…ë‹ˆë‹¤.
ë˜í•œ, ì´ ê¸°ê°„ì˜ í´ë¦½ê³¼ ì£¼ì œì ìœ¼ë¡œ ì—°ê´€ëœ ê³¼ê±° í´ë¦½ë“¤ë„ í•¨ê»˜ ì œê³µë©ë‹ˆë‹¤.

[ì„ íƒ ê¸°ê°„ í´ë¦½ (${clips.length}ê°œ)]
${clipSummaries}

[ê³¼ê±° ì—°ê´€ í´ë¦½ (${relatedClips.length}ê°œ) - ë§¥ë½ ì°¸ê³ ìš©]
${relatedSummaries}

ğŸ“Œ ë¶„ì„ ì§€ì¹¨:
1. ì„ íƒ ê¸°ê°„ ë‚´ í´ë¦½ë“¤ì„ ê°œë³„ì ìœ¼ë¡œ ìš”ì•½í•˜ì§€ ë§ˆì„¸ìš”.
2. ë°˜ë³µ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œ, ë¬¸ì œì˜ì‹, ê´€ì  ë³€í™”ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ 'í•˜ë‚˜ì˜ í° íë¦„'ì„ ë¨¼ì € ë„ì¶œí•˜ì„¸ìš”.
3. ê·¸ íë¦„ì´ ê³¼ê±° í´ë¦½ì˜ ì–´ë–¤ ë§¥ë½ì—ì„œ ì¶œë°œí–ˆëŠ”ì§€ ì—°ê²°í•˜ì„¸ìš”.
4. ì‚¬ìš©ìì˜ ê´€ì‹¬ì‚¬ê°€ ê³¼ê±°ì—” ë¬´ì—‡ì´ì—ˆê³ , ì–´ë–»ê²Œ í™•ì¥/ë³€í˜•ë˜ì—ˆìœ¼ë©°, í˜„ì¬ ì–´ë–¤ ë‹¨ê³„ì— ë„ë‹¬í–ˆëŠ”ì§€ 'ê´€ì‹¬ì‚¬ì˜ ì§„í™”' ê´€ì ì—ì„œ ì„¤ëª…í•˜ì„¸ìš”.

[ì§€ì‹ ë„¤íŠ¸ì›Œí¬ ê´€ì ]
ì´ í´ë¦½ë“¤ì„ í•˜ë‚˜ì˜ 'ì§€ì‹ ë„¤íŠ¸ì›Œí¬'ë¡œ ê°„ì£¼í•˜ì„¸ìš”.
ê° í´ë¦½ì€ ë…¸ë“œ(node)ì´ë©°, ê³µí†µ í‚¤ì›Œë“œ, ë¬¸ì œì˜ì‹, ì¸ìš©, ì£¼ì œ í™•ì¥ì€ ì—£ì§€(edge)ì…ë‹ˆë‹¤.
- ê°€ì¥ ì¤‘ì‹¬ì´ ë˜ëŠ” ë…¸ë“œ(ì£¼ì œ)ëŠ” ë¬´ì—‡ì¸ê°€?
- ì£¼ë³€ë¶€ì—ì„œ ì ì  ì¤‘ì‹¬ìœ¼ë¡œ ì´ë™í•œ ì£¼ì œëŠ” ë¬´ì—‡ì¸ê°€?
- ìµœê·¼ì— ìƒˆë¡­ê²Œ ë“±ì¥í•œ ë…¸ë“œëŠ” ë¬´ì—‡ì´ë©°, ê¸°ì¡´ ì–´ë–¤ ë…¸ë“œì™€ ì—°ê²°ë˜ëŠ”ê°€?

ğŸ“Œ ì¶œë ¥ í˜•ì‹ (Markdown):
## í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìš”ì•½
(3~5ì¤„ë¡œ í•µì‹¬ë§Œ ìš”ì•½)

## ê´€ì‹¬ì‚¬ íë¦„ íƒ€ì„ë¼ì¸
- **ê³¼ê±°**: (ê³¼ê±° í´ë¦½ ê¸°ë°˜ ë°°ê²½)
- **ì „í™˜ì **: (ë³€í™”ì˜ ê³„ê¸°ê°€ ëœ ì½˜í…ì¸ ë‚˜ ì‹œì )
- **í˜„ì¬**: (ìµœê·¼ ì§‘ì¤‘í•˜ê³  ìˆëŠ” ì£¼ì œ)

## ìµœê·¼ ë³€í™”ì˜ íŠ¹ì§•
(ìµœê·¼ ê¸°ê°„ì˜ ë‘ë“œëŸ¬ì§„ íŠ¹ì§• ì„œìˆ )

## ì£¼ëª©í•˜ê³  ìˆëŠ” í•µì‹¬ ì£¼ì œ
1. **ì£¼ì œ 1**: ì„¤ëª…
2. **ì£¼ì œ 2**: ì„¤ëª…
3. **ì£¼ì œ 3**: ì„¤ëª…

## ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ
(ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ ëœ ë‹¤ìŒ ê´€ì‹¬ì‚¬ ì œì•ˆ - ì¶”ì¸¡ ê¸ˆì§€)

âš ï¸ ì£¼ì˜:
- ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
- ì¼ë°˜ì ì¸ íŠ¸ë Œë“œ ì„¤ëª…ì„ í•˜ì§€ ë§ê³ , ì˜¤ì§ ì œê³µëœ í´ë¦½ ê°„ì˜ ê´€ê³„ë§Œ ë¶„ì„í•˜ì„¸ìš”.

[ì‹œìŠ¤í…œ ì•ˆì „ì¥ì¹˜]
ì„ íƒ ê¸°ê°„ì´ ê¸¸ ê²½ìš°, ìµœê·¼ í´ë¦½ì„ ìš°ì„ ì ìœ¼ë¡œ ë¶„ì„í•˜ë˜ ê³¼ê±° í´ë¦½ì€ 'ì—°ê²° ë§¥ë½ ì„¤ëª…'ì—ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
ëª¨ë“  í´ë¦½ì„ ë™ì¼ ë¹„ì¤‘ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ ë§ˆì„¸ìš”.
`
        : `
[Generate Insights Report]

Here is the list of clips saved by the user during the selected period (${startDate} ~ ${endDate}).
Also provided are past clips that are thematically related to this period.

[Selected Period Clips (${clips.length})]
${clipSummaries}

[Related Past Clips (${relatedClips.length}) - For Context]
${relatedSummaries}

ğŸ“Œ Analysis Guidelines:
1. Do NOT summarize clips individually.
2. Identify a "major flow" focusing on recurring keywords, issues, and perspective shifts.
3. Connect this flow to the context from past clips.
4. Explain the "Evolution of Interests": what they were initially, how they expanded/changed, and where they are now.

[Knowledge Network Perspective]
Treat these clips as a 'knowledge network'.
Each clip is a node, and common keywords, issues, citations, topic expansions are edges.
- What is the most central node (topic)?
- Which topics moved from the periphery to the center?
- What new nodes appeared recently, and which existing nodes do they connect to?

ğŸ“Œ Output Format (Markdown):
## Key Insights Summary
(3-5 lines of core summary)

## Interest Timeline
- **Past**: (Background from past clips)
- **Turning Point**: (Content or moment that triggered change)
- **Present**: (Currently focused topics)

## Characteristics of Recent Changes
(Description of notable changes in the recent period)

## Core Topics in Focus
1. **Topic 1**: Description
2. **Topic 2**: Description
3. **Topic 3**: Description

## Suggested Next Steps
(Data-driven suggestions for future interests - no speculation)

âš ï¸ Warning:
- Do NOT generate content not present in the data.
- Do NOT describe general trends, only analyze relationships between provided clips.

[System Safeguard]
If the period is long, prioritize recent clips for analysis and use past clips only for contextual connection.
Do not treat all clips with equal weight.
`;

    if (config.provider === 'openai') {
        return await callOpenAI(config.apiKey, config.model, prompt);
    } else {
        return await callGemini(config.apiKey, config.model, prompt);
    }
};

// Generate AI article
export const generateAIArticle = async (
    clips: any[],
    relatedClips: any[],
    insightSummary: string,
    language: 'ko' | 'en'
): Promise<AIResponse> => {
    const config = getAIConfig();
    if (!config) {
        return { success: false, error: 'AI not configured' };
    }

    const clipSummaries = clips.map((clip, i) =>
        `[Current-${i + 1}] ${clip.title || clip.url}\n   Date: ${new Date(clip.createdAt?.seconds ? clip.createdAt.seconds * 1000 : clip.createdAt).toISOString().split('T')[0]}\n   Keywords: ${(clip.tags || clip.keywords || []).join(', ')}\n   Summary: ${clip.summary || clip.description || ''}`
    ).join('\n\n');

    const relatedSummaries = relatedClips.map((clip, i) =>
        `[Past-${i + 1}] ${clip.title || clip.url}\n   Date: ${new Date(clip.createdAt?.seconds ? clip.createdAt.seconds * 1000 : clip.createdAt).toISOString().split('T')[0]}\n   Keywords: ${(clip.tags || clip.keywords || []).join(', ')}\n   Summary: ${clip.summary || clip.description || ''}`
    ).join('\n\n');

    const prompt = language === 'ko'
        ? `
[ì˜¤ë¦¬ì§€ë„ ì•„í‹°í´ ì‘ì„± ìš”ì²­]

ë‹¤ìŒì€ í•œ ì‚¬ìš©ìê°€ ì¼ì • ê¸°ê°„ ë™ì•ˆ ì €ì¥í•œ ì½˜í…ì¸ ì™€, ê·¸ ì´ì „ë¶€í„° ì¶•ì í•´ì˜¨ ê´€ë ¨ ì½˜í…ì¸ ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë„ì¶œëœ ì¸ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤.

[ì¸ì‚¬ì´íŠ¸ ìš”ì•½]
${insightSummary}

[ì„ íƒ ê¸°ê°„ í´ë¦½ - ìµœê·¼ (${clips.length}ê°œ)]
${clipSummaries}

[ê³¼ê±° ì—°ê´€ í´ë¦½ - ë°°ê²½/ë§¥ë½ (${relatedClips.length}ê°œ)]
${relatedSummaries}

ğŸ“Œ ì•„í‹°í´ ì‘ì„± ì§€ì¹¨:
1. ë‹¨ì¼ ê²Œì‹œë¬¼ ì†Œê°œ í˜•íƒœë¡œ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”.
2. í•˜ë‚˜ì˜ ì£¼ì œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ 'ìƒê°ì˜ ì¶•ì  ê³¼ì •'ì´ ë“œëŸ¬ë‚˜ì•¼ í•©ë‹ˆë‹¤.
3. ê³¼ê±° ì½˜í…ì¸ ëŠ” ë°°ê²½ê³¼ ë§¥ë½ ì„¤ëª…ì— í™œìš©í•˜ì„¸ìš”.
4. ì‹œê°„ì´ íë¥´ë©° ê´€ì ì´ ì–´ë–»ê²Œ ë³€í™”í–ˆëŠ”ì§€ë¥¼ ëª…í™•íˆ ë³´ì—¬ì£¼ì„¸ìš”.
5. ë…ìëŠ” "ì´ ì£¼ì œì— ëŒ€í•´ ê¹Šì´ ê³ ë¯¼í•´ì˜¨ ì‚¬ëŒì˜ ê¸°ë¡"ì„ ì½ëŠ” ëŠë‚Œì„ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤.

ğŸ“Œ ë¬¸ì²´ ë° êµ¬ì¡° (Markdown):
# (í¥ë¯¸ë¡œìš´ ì œëª©)

## ì„œë¡ : ìµœê·¼ì˜ ë§¥ë½
(ì™œ ì´ ì£¼ì œê°€ ì§€ê¸ˆ ì¤‘ìš”í•´ì¡ŒëŠ”ì§€)

## 1. ìƒê°ì˜ ì¶œë°œì 
(ê³¼ê±° ê´€ì‹¬ì‚¬ ë° ë°°ê²½ - ê³¼ê±° í´ë¦½ ì°¸ì¡°)

## 2. ê´€ì ì˜ ë³€í™”ì™€ í™•ì¥
(ìµœê·¼ ì½˜í…ì¸ ë¥¼ í†µí•´ ì•Œê²Œ ëœ ìƒˆë¡œìš´ ì‚¬ì‹¤ì´ë‚˜ ì‹œê°)

## 3. í˜„ì¬ì˜ ë¬¸ì œì˜ì‹
(ì‚¬ìš©ìê°€ ì§€ê¸ˆ ê°€ì¥ ì§‘ì¤‘í•˜ê³  ìˆëŠ” í¬ì¸íŠ¸)

## ê²°ë¡ : ì§ˆë¬¸ê³¼ ì „ë§
(ì •ë‹µ ì œì‹œ ê¸ˆì§€ - ë‹¤ìŒìœ¼ë¡œ ì´ì–´ì§ˆ ì¢‹ì€ ì§ˆë¬¸ ì œì‹œ)

âš ï¸ ê¸ˆì§€ ì‚¬í•­:
- ì‚¬ì‹¤ì„ ì¼ë°˜í™”í•˜ì§€ ë§ˆì„¸ìš”.
- ì œê³µë˜ì§€ ì•Šì€ ì •ë³´ë¡œ ë…¼ë¦¬ë¥¼ í™•ì¥í•˜ì§€ ë§ˆì„¸ìš”.
- ë§ˆì¼€íŒ… ë¬¸êµ¬ì²˜ëŸ¼ ì“°ì§€ ë§ˆì„¸ìš”.
- ë°˜ë“œì‹œ ì œê³µëœ í´ë¦½ ë°ì´í„°ë§Œ í™œìš©í•˜ì„¸ìš”.
`
        : `
[Generate Original Article]

Based on the content saved by a user over a period and related past content, write an original article.

[Insight Summary]
${insightSummary}

[Selected Period Clips - Recent (${clips.length})]
${clipSummaries}

[Related Past Clips - Background/Context (${relatedClips.length})]
${relatedSummaries}

ğŸ“Œ Writing Guidelines:
1. Do NOT write as a list of individual links.
2. Show the "accumulation of thought" around a central theme.
3. Use past content for background and context.
4. Clearly show how perspectives have changed over time.
5. Create a feeling of reading "records of someone who has thought deeply about this topic".

ğŸ“Œ Structure (Markdown):
# (Compelling Title)

## Introduction: Recent Context
(Why this topic has become important now)

## 1. Origin of Thought
(Past interests and background - reference past clips)

## 2. Shift in Perspective
(New facts or viewpoints discovered through recent content)

## 3. Current Focus
(The point the user is most focused on now)

## Conclusion: Questions for the Future
(Do NOT provide answers - present good questions for the next step)

âš ï¸ Prohibited:
- Do NOT generalize facts.
- Do NOT extend logic with information not provided.
- Do NOT write like marketing copy.
- Use ONLY the provided clip data.
`;

    if (config.provider === 'openai') {
        return await callOpenAI(config.apiKey, config.model, prompt);
    } else {
        return await callGemini(config.apiKey, config.model, prompt);
    }
};

// AI Chat
export const sendAIChat = async (message: string, context: string, language: 'ko' | 'en'): Promise<AIResponse> => {
    const config = getAIConfig();
    if (!config) {
        return { success: false, error: 'AI not configured' };
    }

    const prompt = language === 'ko'
        ? `ì‚¬ìš©ìì˜ ì €ì¥ëœ ì½˜í…ì¸ ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì €ì¥ëœ ì½˜í…ì¸  ìš”ì•½:
${context}

ì‚¬ìš©ì ì§ˆë¬¸: ${message}

ê°„ê²°í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.`
        : `Please answer the question based on the user's saved content.

Saved content summary:
${context}

User question: ${message}

Please provide a concise and helpful answer.`;

    if (config.provider === 'openai') {
        return await callOpenAI(config.apiKey, config.model, prompt);
    } else {
        return await callGemini(config.apiKey, config.model, prompt);
    }
};
